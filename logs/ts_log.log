2024-07-13T00:00:26,986 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,994 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:227.4813346862793|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,995 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:704.0147552490234|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,995 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:75.6|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,995 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.87890625|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,996 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:54.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,996 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,996 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:2887.98828125|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,996 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13133.88671875|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:00:26,997 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:82.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843226
2024-07-13T00:01:27,029 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,029 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:227.48221969604492|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,029 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:704.0138702392578|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,029 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:75.6|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,030 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.634765625|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,030 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:39.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,030 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,031 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:2462.46875|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,031 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13562.55078125|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:27,032 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:84.7|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843287
2024-07-13T00:01:41,109 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-07-13T00:01:41,109 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-07-13T00:01:41,112 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-07-13T00:01:41,112 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-07-13T00:01:41,170 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
2024-07-13T00:01:41,170 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
2024-07-13T00:01:41,321 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages
Current directory: D:\Proyectos\tenpo\doubleit
Temp directory: C:\Users\Rodrigo\AppData\Local\Temp
Metrics config path: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 4006 M
Python executable: D:\Programs\Anaconda3\envs\tenpo2\python.exe
Config file: config/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: D:\Proyectos\tenpo\doubleit\model_store
Initial Models: doubleit_model.mar
Log dir: D:\Proyectos\tenpo\doubleit\logs
Metrics dir: D:\Proyectos\tenpo\doubleit\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: D:\Proyectos\tenpo\doubleit\model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-07-13T00:01:41,321 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages
Current directory: D:\Proyectos\tenpo\doubleit
Temp directory: C:\Users\Rodrigo\AppData\Local\Temp
Metrics config path: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 4006 M
Python executable: D:\Programs\Anaconda3\envs\tenpo2\python.exe
Config file: config/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: D:\Proyectos\tenpo\doubleit\model_store
Initial Models: doubleit_model.mar
Log dir: D:\Proyectos\tenpo\doubleit\logs
Metrics dir: D:\Proyectos\tenpo\doubleit\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: D:\Proyectos\tenpo\doubleit\model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-07-13T00:01:41,327 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-07-13T00:01:41,327 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-07-13T00:01:41,341 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: doubleit_model.mar
2024-07-13T00:01:41,341 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: doubleit_model.mar
2024-07-13T00:01:41,369 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model doubleit_model
2024-07-13T00:01:41,369 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model doubleit_model
2024-07-13T00:01:41,370 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model doubleit_model
2024-07-13T00:01:41,370 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model doubleit_model
2024-07-13T00:01:41,370 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model doubleit_model loaded.
2024-07-13T00:01:41,370 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model doubleit_model loaded.
2024-07-13T00:01:41,370 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: doubleit_model, count: 1
2024-07-13T00:01:41,370 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: doubleit_model, count: 1
2024-07-13T00:01:41,378 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2024-07-13T00:01:41,378 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2024-07-13T00:01:41,378 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [D:\Programs\Anaconda3\envs\tenpo2\python.exe, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000, --metrics-config, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml]
2024-07-13T00:01:41,378 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [D:\Programs\Anaconda3\envs\tenpo2\python.exe, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000, --metrics-config, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml]
2024-07-13T00:01:41,434 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2024-07-13T00:01:41,434 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2024-07-13T00:01:41,434 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2024-07-13T00:01:41,434 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2024-07-13T00:01:41,435 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2024-07-13T00:01:41,435 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2024-07-13T00:01:41,436 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2024-07-13T00:01:41,436 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2024-07-13T00:01:41,436 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-07-13T00:01:41,436 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-07-13T00:01:41,627 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-07-13T00:01:41,627 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-07-13T00:01:42,253 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,255 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:227.4822006225586|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,256 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:704.0138893127441|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,257 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:75.6|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,257 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.8626302083333334|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,257 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:53.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,258 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:5.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,258 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:2631.1640625|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,258 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:13392.6015625|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,259 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:83.6|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,845 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Listening on addr:port: 127.0.0.1:9000
2024-07-13T00:01:42,853 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Successfully loaded D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml.
2024-07-13T00:01:42,854 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - [PID]19812
2024-07-13T00:01:42,854 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Torch worker started.
2024-07-13T00:01:42,854 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Python runtime: 3.9.0
2024-07-13T00:01:42,855 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-doubleit_model_1.0 State change null -> WORKER_STARTED
2024-07-13T00:01:42,855 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-doubleit_model_1.0 State change null -> WORKER_STARTED
2024-07-13T00:01:42,858 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2024-07-13T00:01:42,858 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /127.0.0.1:9000
2024-07-13T00:01:42,865 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Connection accepted: ('127.0.0.1', 9000).
2024-07-13T00:01:42,868 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1720843302868
2024-07-13T00:01:42,868 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1720843302868
2024-07-13T00:01:42,870 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1720843302870
2024-07-13T00:01:42,870 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1720843302870
2024-07-13T00:01:42,892 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - model_name: doubleit_model, batchSize: 1
2024-07-13T00:01:42,967 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - OpenVINO is not enabled
2024-07-13T00:01:42,968 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2024-07-13T00:01:42,968 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2024-07-13T00:01:42,987 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117
2024-07-13T00:01:42,987 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117
2024-07-13T00:01:42,988 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-doubleit_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-07-13T00:01:42,988 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-doubleit_model_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2024-07-13T00:01:42,988 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1613.0|#WorkerName:W-9000-doubleit_model_1.0,Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:42,988 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843302
2024-07-13T00:01:43,157 [INFO ] nioEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:doubleit_model,model_version:default|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843303
2024-07-13T00:01:43,159 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1720843303159
2024-07-13T00:01:43,159 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1720843303159
2024-07-13T00:01:43,160 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1720843303160
2024-07-13T00:01:43,160 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1720843303160
2024-07-13T00:01:43,162 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Backend received inference at: 1720843303
2024-07-13T00:01:43,191 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Datos recibidos: [{'body': [[1, -1, 2, 3, 4], [2, 3, 6]]}]
2024-07-13T00:01:43,191 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Datos preprocesados: [[1, -1, 2, 3, 4], [2, 3, 6]]
2024-07-13T00:01:43,191 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Error durante la inferencia: expected sequence of length 5 at dim 1 (got 3)
2024-07-13T00:01:43,191 [INFO ] W-9000-doubleit_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:29.22|#ModelName:doubleit_model,Level:Model|#type:GAUGE|#hostname:LAPTOP-9SVIIN5J,1720843303,bf1e4664-652a-454e-a513-84de01d40864, pattern=[METRICS]
2024-07-13T00:01:43,191 [INFO ] W-9000-doubleit_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:29.22|#ModelName:doubleit_model,Level:Model|#type:GAUGE|#hostname:LAPTOP-9SVIIN5J,1720843303,bf1e4664-652a-454e-a513-84de01d40864, pattern=[METRICS]
2024-07-13T00:01:43,192 [INFO ] W-9000-doubleit_model_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1]:54069 "POST /predictions/doubleit_model HTTP/1.1" 200 36
2024-07-13T00:01:43,192 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:29.22|#ModelName:doubleit_model,Level:Model|#hostname:LAPTOP-9SVIIN5J,requestID:bf1e4664-652a-454e-a513-84de01d40864,timestamp:1720843303
2024-07-13T00:01:43,194 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843303
2024-07-13T00:01:43,195 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:32674.5|#model_name:doubleit_model,model_version:default|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843303
2024-07-13T00:01:43,195 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:233.4|#model_name:doubleit_model,model_version:default|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843303
2024-07-13T00:01:43,195 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 233400, Backend time ns: 35545600
2024-07-13T00:01:43,195 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 233400, Backend time ns: 35545600
2024-07-13T00:01:43,195 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843303
2024-07-13T00:01:43,195 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 30
2024-07-13T00:01:43,195 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 30
2024-07-13T00:01:43,196 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:7.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843303
2024-07-13T00:01:54,186 [INFO ] nioEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:doubleit_model,model_version:default|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843314
2024-07-13T00:01:54,186 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1720843314186
2024-07-13T00:01:54,186 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT repeats 1 to backend at: 1720843314186
2024-07-13T00:01:54,186 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1720843314186
2024-07-13T00:01:54,186 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1720843314186
2024-07-13T00:01:54,187 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Backend received inference at: 1720843314
2024-07-13T00:01:54,223 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Datos recibidos: [{'body': [2, 3, 6]}]
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Datos preprocesados: [2, 3, 6]
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0 ACCESS_LOG - /[0:0:0:0:0:0:0:1]:54069 "POST /predictions/doubleit_model HTTP/1.1" 200 39
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Tensor convertido: tensor([[2., 3., 6.]])
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_LOG - Salida: [4.0, 6.0, 12.0]
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843314
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:36.0|#ModelName:doubleit_model,Level:Model|#type:GAUGE|#hostname:LAPTOP-9SVIIN5J,1720843314,2df06225-829b-406a-bcaa-1e3f10e39850, pattern=[METRICS]
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:36.0|#ModelName:doubleit_model,Level:Model|#type:GAUGE|#hostname:LAPTOP-9SVIIN5J,1720843314,2df06225-829b-406a-bcaa-1e3f10e39850, pattern=[METRICS]
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:37704.9|#model_name:doubleit_model,model_version:default|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843314
2024-07-13T00:01:54,224 [INFO ] W-9000-doubleit_model_1.0-stdout MODEL_METRICS - PredictionTime.ms:36.0|#ModelName:doubleit_model,Level:Model|#hostname:LAPTOP-9SVIIN5J,requestID:2df06225-829b-406a-bcaa-1e3f10e39850,timestamp:1720843314
2024-07-13T00:01:54,225 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:70.5|#model_name:doubleit_model,model_version:default|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843314
2024-07-13T00:01:54,225 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 70500, Backend time ns: 38772200
2024-07-13T00:01:54,225 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.job.RestJob - Waiting time ns: 70500, Backend time ns: 38772200
2024-07-13T00:01:54,225 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843314
2024-07-13T00:01:54,225 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 37
2024-07-13T00:01:54,225 [INFO ] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 37
2024-07-13T00:01:54,225 [INFO ] W-9000-doubleit_model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720843314
2024-07-13T00:36:08,827 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-07-13T00:36:08,827 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-07-13T00:36:08,832 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-07-13T00:36:08,832 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-07-13T00:36:08,880 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
2024-07-13T00:36:08,880 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
2024-07-13T00:36:09,057 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages
Current directory: D:\Proyectos\tenpo\doubleit
Temp directory: C:\Users\Rodrigo\AppData\Local\Temp
Metrics config path: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 4006 M
Python executable: D:\Programs\Anaconda3\envs\tenpo2\python.exe
Config file: config/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: D:\Proyectos\tenpo\doubleit\model_store
Initial Models: doubleit_model.mar
Log dir: D:\Proyectos\tenpo\doubleit\logs
Metrics dir: D:\Proyectos\tenpo\doubleit\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: D:\Proyectos\tenpo\doubleit\model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-07-13T00:36:09,057 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.11.0
TS Home: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages
Current directory: D:\Proyectos\tenpo\doubleit
Temp directory: C:\Users\Rodrigo\AppData\Local\Temp
Metrics config path: D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 12
Max heap size: 4006 M
Python executable: D:\Programs\Anaconda3\envs\tenpo2\python.exe
Config file: config/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://127.0.0.1:8082
Model Store: D:\Proyectos\tenpo\doubleit\model_store
Initial Models: doubleit_model.mar
Log dir: D:\Proyectos\tenpo\doubleit\logs
Metrics dir: D:\Proyectos\tenpo\doubleit\logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: LOG
Disable system metrics: false
Workflow Store: D:\Proyectos\tenpo\doubleit\model_store
CPP log config: N/A
Model config: N/A
System metrics command: default
2024-07-13T00:36:09,066 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-07-13T00:36:09,066 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-07-13T00:36:09,084 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: doubleit_model.mar
2024-07-13T00:36:09,084 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: doubleit_model.mar
2024-07-13T00:36:09,117 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model doubleit_model
2024-07-13T00:36:09,117 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model doubleit_model
2024-07-13T00:36:09,117 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model doubleit_model
2024-07-13T00:36:09,117 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model doubleit_model
2024-07-13T00:36:09,117 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model doubleit_model loaded.
2024-07-13T00:36:09,117 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model doubleit_model loaded.
2024-07-13T00:36:09,117 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: doubleit_model, count: 1
2024-07-13T00:36:09,117 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: doubleit_model, count: 1
2024-07-13T00:36:09,127 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2024-07-13T00:36:09,127 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: NioServerSocketChannel.
2024-07-13T00:36:09,127 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [D:\Programs\Anaconda3\envs\tenpo2\python.exe, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000, --metrics-config, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml]
2024-07-13T00:36:09,127 [DEBUG] W-9000-doubleit_model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [D:\Programs\Anaconda3\envs\tenpo2\python.exe, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages\ts\model_service_worker.py, --sock-type, tcp, --port, 9000, --metrics-config, D:\Programs\Anaconda3\envs\tenpo2\Lib\site-packages/ts/configs/metrics.yaml]
2024-07-13T00:36:09,195 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2024-07-13T00:36:09,195 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2024-07-13T00:36:09,195 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2024-07-13T00:36:09,195 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: NioServerSocketChannel.
2024-07-13T00:36:09,197 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2024-07-13T00:36:09,197 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2024-07-13T00:36:09,197 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2024-07-13T00:36:09,197 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: NioServerSocketChannel.
2024-07-13T00:36:09,198 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-07-13T00:36:09,198 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2024-07-13T00:36:09,439 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-07-13T00:36:09,439 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2024-07-13T00:36:10,593 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,596 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:227.48235321044922|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,597 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:704.0137367248535|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,598 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:75.6|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,599 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUtilization.Percent:0.5859375|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,600 [INFO ] pool-3-thread-1 TS_METRICS - GPUMemoryUsed.Megabytes:36.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,600 [INFO ] pool-3-thread-1 TS_METRICS - GPUUtilization.Percent:0.0|#Level:Host,DeviceId:0|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,600 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:53.43359375|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,601 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:15968.58984375|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
2024-07-13T00:36:10,602 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:99.7|#Level:Host|#hostname:LAPTOP-9SVIIN5J,timestamp:1720845370
